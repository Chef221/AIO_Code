{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56c3d92c",
   "metadata": {},
   "source": [
    "DataTable Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7347fc44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TV Percentage: 73.206%\n",
      "Radio Percentage: 11.582%\n",
      "Newspaper Percentage: 15.212%\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    + Tabular data: Data that is organized in a table with rows and columns. We can think\n",
    "of it as a 2D array, list of lists...\n",
    "    + Rows (records, samples): Observation (cases)\n",
    "    + Columns (fields, features): Attributes of observations\n",
    "- How to interact tabular data in Python ?\n",
    "    + Pandas library: A fast, powerful and easy to use open source data analysis and\n",
    "manipulation tool\n",
    "'''\n",
    "import pandas as pd\n",
    "\n",
    "# Read .csv file (read_csv() returns a DataFrame)\n",
    "url = \"https://raw.githubusercontent.com/justmarkham/scikit-learn-videos/master/data/Advertising.csv\"\n",
    "advertising_df = pd.read_csv(url, index_col=0)\n",
    "\n",
    "\n",
    "# Convert DataFrame to Python List of Lists\n",
    "# Currently, we don't often use DataFrame -> Convert to List of Lists\n",
    "advertising_list = advertising_df.values.tolist()\n",
    "\n",
    "\n",
    "# Action with list\n",
    "tv_sum = sum([lst[0] for lst in advertising_list])\n",
    "radio_sum = sum([lst[1] for lst in advertising_list])\n",
    "newspaper_sum = sum([lst[2] for lst in advertising_list])\n",
    "sales_sum = sum([lst[3] for lst in advertising_list])\n",
    "\n",
    "# Find sum of columns from row 2 to row 11\n",
    "start_idx = 1\n",
    "end_idx = 11\n",
    "sliced_list = advertising_list[start_idx:end_idx]\n",
    "tv_sum = sum([lst[0] for lst in sliced_list])\n",
    "radio_sum = sum([lst[1] for lst in sliced_list])\n",
    "newspaper_sum = sum([lst[2] for lst in sliced_list])\n",
    "sales_sum = sum([lst[3] for lst in sliced_list])\n",
    "\n",
    "# Find median\n",
    "def median(lst):\n",
    "    sorted_lst = sorted(lst)\n",
    "    n = len(lst)\n",
    "    mid = n // 2\n",
    "    if n % 2 == 0:\n",
    "        median = (sorted_lst[mid] + sorted_lst[mid - 1]) / 2\n",
    "    else:\n",
    "        median = sorted_lst[mid] \n",
    "    return median\n",
    "\n",
    "tv_median = median([lst[0] for lst in advertising_list])\n",
    "radio_median = median([lst[1] for lst in advertising_list])\n",
    "newspaper_median = median([lst[2] for lst in advertising_list])\n",
    "sales_median = median([lst[3] for lst in advertising_list])\n",
    "\n",
    "# Find contribution percentage\n",
    "tv_lst = [lst[0] for lst in advertising_list]\n",
    "radio_lst = [lst[1] for lst in advertising_list]\n",
    "newspaper_lst = [lst[2] for lst in advertising_list]\n",
    "\n",
    "total_budget = sum(tv_lst) + sum(radio_lst) + sum(newspaper_lst)\n",
    "\n",
    "tv_percentage = (sum(tv_lst) / total_budget) * 100\n",
    "radio_percentage = (sum(radio_lst) / total_budget) * 100\n",
    "newspaper_percentage = (sum(newspaper_lst) / total_budget) * 100\n",
    "\n",
    "print(f'TV Percentage: {tv_percentage:.3f}%')\n",
    "print(f'Radio Percentage: {radio_percentage:.3f}%')\n",
    "print(f'Newspaper Percentage: {newspaper_percentage:.3f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc363ed",
   "metadata": {},
   "source": [
    "Text Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "965b89fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- In general, we don't often use raw text when computing in some tasks -> Need a better\n",
    "representation for text\n",
    "- Index-based Encoding:\n",
    "    + Use a dictionary with key = word and value = index to transform text -> list\n",
    "    + Step 1: Corpus (List of paragraphs) -> Text Normalization -> Create Dictionary\n",
    "    + Step 2: A string -> Text Normalization -> Vectorize (+ Dictionary) -> New representation\n",
    "'''\n",
    "\n",
    "'''\n",
    "Text Normalization Introduction\n",
    "- Problem: \n",
    "    + Documents contain unnecessary string (information)\n",
    "    + Not well-present natural language\n",
    "'''\n",
    "\n",
    "# Text Normalization: Lowercasing\n",
    "import string\n",
    "# remove_characters sẽ chứa một chuỗi dài gồm các kỳ tự Tab, dấu ngoặc kép và tất cả dấu câu\n",
    "remove_characters = '\\t\"\"' + string.punctuation\n",
    "def text_normalization(text):\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = text.replace('\\n',' ')\n",
    "    for char in remove_characters:\n",
    "        text = text.replace(char, '')\n",
    "    return text\n",
    "\n",
    "# Create Dictionary\n",
    "# Given a list of paragraphs -> Get a list of unique words\n",
    "def create_dictionary(corpus):\n",
    "    dictionary = []\n",
    "    for paragraph in corpus:\n",
    "        paragraph = text_normalization(paragraph)\n",
    "        tokens = paragraph.split()\n",
    "        for token in tokens:\n",
    "            if token not in dictionary:\n",
    "                dictionary.append(token)\n",
    "    return dictionary\n",
    "\n",
    "# Create New Text Representation\n",
    "def vectorize(text, dictionary, unknown_token_id):\n",
    "    text = text_normalization(text)\n",
    "    tokens = text.split()\n",
    "    vector = [\n",
    "        dictionary.index(token) \n",
    "            if token in dictionary else unknown_token_id \n",
    "                for token in tokens\n",
    "    ]\n",
    "    return vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd3734b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 'Artificial Intelligence is FASCINATING!!!'\n",
      "Cleaned : artificial intelligence is fascinating\n",
      "------------------------------\n",
      "Original: 'Python\\tis\\ta\\tgreat\\tlanguage.'\n",
      "Cleaned : pythonisagreatlanguage\n",
      "------------------------------\n",
      "Original: \"Data Science: The \\n'Sexiest' Job of the 21st Century.\"\n",
      "Cleaned : data science the  sexiest job of the 21st century\n",
      "------------------------------\n",
      "Original: 'Hello, world! Are you ready for AI?'\n",
      "Cleaned : hello world are you ready for ai\n",
      "------------------------------\n",
      "Original: 'Natural Language Processing (NLP) includes: tokenization, stemming, & lemmatization.'\n",
      "Cleaned : natural language processing nlp includes tokenization stemming  lemmatization\n",
      "------------------------------\n",
      "Original: '   Machine Learning... requires    math!   '\n",
      "Cleaned : machine learning requires    math\n",
      "------------------------------\n",
      "['artificial', 'intelligence', 'is', 'fascinating', 'pythonisagreatlanguage', 'data', 'science', 'the', 'sexiest', 'job', 'of', '21st', 'century', 'hello', 'world', 'are', 'you', 'ready', 'for', 'ai', 'natural', 'language', 'processing', 'nlp', 'includes', 'tokenization', 'stemming', 'lemmatization', 'machine', 'learning', 'requires', 'math']\n",
      "Text: Python is fascinating\n",
      "Vector: [-1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    \"Artificial Intelligence is FASCINATING!!!\",\n",
    "    \"Python\\tis\\ta\\tgreat\\tlanguage.\", # Chứa tab\n",
    "    \"Data Science: The \\n'Sexiest' Job of the 21st Century.\",\n",
    "    \"Hello, world! Are you ready for AI?\",\n",
    "    \"Natural Language Processing (NLP) includes: tokenization, stemming, & lemmatization.\",\n",
    "    \"   Machine Learning... requires    math!   \"\n",
    "]\n",
    "cleaned_corpus = [text_normalization(text) for text in corpus]\n",
    "for i, sentence in enumerate(cleaned_corpus):\n",
    "    print(f\"Original: {repr(corpus[i])}\")\n",
    "    print(f\"Cleaned : {sentence}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "a_dict = create_dictionary(corpus)\n",
    "\n",
    "unknown_id = -1\n",
    "print(a_dict)\n",
    "text_1 = \"Python is fascinating\"\n",
    "vector_1 = vectorize(text_1, a_dict, unknown_id)\n",
    "print(f\"Text: {text_1}\")\n",
    "print(f\"Vector: {vector_1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd13db5f",
   "metadata": {},
   "source": [
    "Data Crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3327a9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- Introduction: \n",
    "    + We need dataset\n",
    "=> Problem:\n",
    "    + Not enough data\n",
    "    + New class that doesn't have dataset\n",
    "    + Improve performance\n",
    "=> Solution:\n",
    "    + We collect more data\n",
    "=> Webpages -> Data Crawling / Web Scraping (Tools/Programs that collecting data from webpages) \n",
    "'''\n",
    "\n",
    "'''\n",
    "Motivation:\n",
    "- A webpage's content is represent is something called HTML\n",
    "=> General steps to crawling\n",
    "URL -> Web Crawler App -> Log file\n",
    "\n",
    "- Web Crawler App:\n",
    "    + Request -> Server -> HTML -> Link Extraction App -> URL List\n",
    "\n",
    "- HTML: The standard markup language for documents designed to be displayed in a web browser\n",
    "'''\n",
    "\n",
    "'''\n",
    "- Selenium Package: Used to automate web browser interaction from Python\n",
    "- Things to extract: Article, Author, Abstract, Body\n",
    "- Get an article URL -> Crawler -> Extraction data\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7039cf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selenium Example\n",
    "# Step 1: Initialize a browser and access to the website\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import requests\n",
    "\n",
    "from tqdm import tqdm\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium import webdriver\n",
    "\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument(\"start-maximized\")\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "driver = webdriver.Chrome(\n",
    "    service = Service(ChromeDriverManager().install()),\n",
    "    options = chrome_options\n",
    ")\n",
    "\n",
    "url = 'https://www.python.org/'\n",
    "driver.get(url)\n",
    "# print(driver.page_source)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f76fec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Program pipeline:\n",
    "1. Create a browser\n",
    "2. Request HTML from table website\n",
    "3. Get list of articles\n",
    "4. Request HTML from article\n",
    "5. Extract content\n",
    "6. Save content and move to next article\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
